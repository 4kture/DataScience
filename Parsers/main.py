import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import os

headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

base_url = 'https://books.toscrape.com/catalogue/category/books_1/'

os.makedirs('pages_data', exist_ok=True)

for count in range(25, 26):
    url = f'{base_url}page-{count}.html'
    print(f'üõ†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}..')
    print('-' * 80 + '\n')

    response = requests.get(url, headers=headers)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, 'lxml')

    if response.status_code != 200:
        print('‚õî –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.')
        break

    books = soup.find_all('article', class_='product_pod')
    all_books = []

    for book in books:
        try:
            relative_link = book.find('h3').find('a')['href']
            detail_url = 'https://books.toscrape.com/catalogue/' + relative_link.replace('../../', '')

            detail_response = requests.get(detail_url, headers=headers)
            detail_response.encoding = 'utf-8'
            detail_soup = BeautifulSoup(detail_response.text, 'lxml')

            category = None
            breadcrumb = detail_soup.select('ul.breadcrumb li')
            if len(breadcrumb) >= 3:
                category = breadcrumb[2].text.strip()

            table = {}
            for row in detail_soup.select('table.table-striped tr'):
                key = row.find('th').text.strip()
                value = row.find('td').text.strip()
                table[key] = value

            description_header = detail_soup.find('div', id='product_description')
            if description_header:
                description_tag = description_header.find_next_sibling('p')
                description = description_tag.text.strip()
            else:
                description = None

            rating_tag = detail_soup.find('p', class_='star-rating')
            rating = None
            if rating_tag:
                classes = rating_tag.get('class', [])
                for cls in classes:
                    if cls in ['One', 'Two', 'Three', 'Four', 'Five']:
                        rating_map = {
                            'One': 1,
                            'Two': 2,
                            'Three': 3,
                            'Four': 4,
                            'Five': 5
                        }
                        rating = rating_map.get(cls)
                        break

            name = detail_soup.find('h1').text
            upc = table.get('UPC')
            product_type = table.get('Product Type')
            price_excl_tax = table.get('Price (excl. tax)')
            price_incl_tax = table.get('Price (incl. tax)')
            tax = table.get('Tax')
            availability = table.get('Availability')
            reviews = table.get('Number of reviews')
            image_url = 'https://books.toscrape.com/' + detail_soup.find('img')['src'].replace('../../', '')

            print(f'üìñ –ù–∞–∑–≤–∞–Ω–∏–µ: {name}')
            print(f'üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}')
            print(f'üÜî UPC (–ê–π–¥–∏): {upc}')
            print(f'üìù –û–ø–∏—Å–∞–Ω–∏–µ: {description}')
            print(f'‚≠ê –†–µ–π—Ç–∏–Ω–≥: {rating}')
            print(f'üìö –¢–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞: {product_type}')
            print(f'üíµ –¶–µ–Ω–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞: {price_excl_tax}')
            print(f'üí∞ –¶–µ–Ω–∞ —Å –Ω–∞–ª–æ–≥–æ–º: {price_incl_tax}')
            print(f'üìä –ù–∞–ª–æ–≥: {tax}')
            print(f'üì¶ –ù–∞–ª–∏—á–∏–µ: {availability}')
            print(f'üó≥Ô∏è –ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤: {reviews}')
            print(f'üñºÔ∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞: {image_url}')
            print(f'üîó –°—Å—ã–ª–∫–∞: {detail_url}')
            print('-' * 80 + '\n')

            all_books.append({
                '–ù–∞–∑–≤–∞–Ω–∏–µ': name,
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                'UPC (–ê–π–¥–∏)': upc,
                '–û–ø–∏—Å–∞–Ω–∏–µ': description,
                '–†–µ–π—Ç–∏–Ω–≥': rating,
                '–¢–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞': product_type,
                '–¶–µ–Ω–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞': price_excl_tax,
                '–¶–µ–Ω–∞ —Å –Ω–∞–ª–æ–≥–æ–º:': price_incl_tax,
                '–ù–∞–ª–æ–≥': tax,
                '–ù–∞–ª–∏—á–∏–µ': availability,
                '–ö–æ–ª-–≤–æ –æ—Ç–∑—ã–≤–æ–≤': reviews,
                '–ö–∞—Ä—Ç–∏–Ω–∫–∞': image_url,
                '–°—Å—ã–ª–∫–∞': detail_url
            })

        except Exception as e:
            print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–Ω–∏–≥–∏: {e}')
            continue

    page_filename = f'pages_data/books_page_{count}.csv'
    pd.DataFrame(all_books).to_csv(page_filename, index=False, encoding='utf-8-sig')
    print(f'‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {count} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {page_filename}\n')

print('‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!')
